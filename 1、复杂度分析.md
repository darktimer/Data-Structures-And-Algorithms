[TOC]

## 为什么要复杂度分析？

通过统计、监控得到算法执行时间和占内存大小，这叫**事后统计法**。这种统计方法有很大局限性，因为**测试结果非常依赖测试环境，**测试环境中的硬件不同会对测试结果产生很大影响，另外，**数据规模**也会影响测试结果，这样的例子在排序算法中会有明显的对比。所以需要一个不需要具体测试数据的办法来粗略估计算法执行效率。

## 算法执行效率

可以粗略的用算法代码执行时间来表示执行效率。**代码执行时间 T(n) **与每行代码的**执行次数**成正比。

<img src="https://static001.geekbang.org/resource/image/22/ef/22900968aa2b190072c985a08b0e92ef.png" alt="img" style="zoom:50%;" />

T(n) 表示代码执行的时间；n 表示数据规模的大小；**f(n) 表示每行代码执行的次数总和。**因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。

为什么 f(n) 加粗，因为这个表示执行次数总和，O() 的是次数，不是 n 数据规模，这个会在分析计算时经常搞混。

时间复杂度并不表示代码的真正执行时间，**而是表示代码执行时间随数据规模增长的变化趋势**，所以全称叫**渐进时间复杂度**。

类似 T(n) = O(2n+2)， T(n) = O(2n^2^+2n+3) 这样的式子，低阶、常量、系数三者不会左右增长趋势，可以忽略，只需要考虑最大阶，有点类似极限，所以可以记为：T(n) = O(n)， T(n) = O(n^2^) 。

## 时间复杂度分析

要点大体有 3 个：关注循环执行次数最多代码，加法法则，乘法法则。

**执行次数最多的代码：**上面提到在公式中，低阶、常量、系数对复杂度没有影响。

**加法法则：**上面说低阶、常量、系数对复杂度没有影响，这个可能会不好体会，就是这些只是会影响代码的执行时间，比如数据规模大一点或小一点，那么常量或者系数或者低阶会变化，代码执行时间会变长或者变短，总归是个确定的值，如果数据规模 n 趋于无限，那么常量这些都可以忽略了，真的跟极限的概念很像。但回到时间复杂度的概念来说，关注的是执行效率和数据规模增长的变化趋势，不管常量系数什么的怎么变化，都可以忽略掉，因为对时间复杂度没有影响。真正影响的是最高阶。

T~1~(n)=O(f(n))，T~2~(n)=O(g(n))；那么 T(n)=T~1~(n)+T~2~(n) = max(O(f(n)), O(g(n)))  = O(max(f(n), g(n)))

也跟水桶原理差不多，桶能装多少水不取决最高的木板，而是看哪块木板最低，水也就能装那么多。

**乘法法则：**有嵌套的代码，它的时间复杂度是嵌套内、外代码复杂度的乘积。比如一段代码执行次数是 n，在这 n 次执行中，每一次里，又有一段代码要执行 n 次，所以这段嵌套总共执行了 n * n 次，复杂度就是O(n^2^)。

T(n) = T~1~(n) * T~2~(n) = O(n*n) = O(n^2^)

## 常见时间复杂度

O(1) < O(log n) < O(n) < O(n log n) < O(n^2^)

<img src="https://static001.geekbang.org/resource/image/49/04/497a3f120b7debee07dc0d03984faf04.jpg" alt="img" style="zoom:50%;" />

**O(1)：**常量级时间复杂度的表示方法，**并不表示**代码只有一行或只执行一次。哪怕代码成千上万行，没循环，没递归，一般情况下时间复杂度也是O(1)。

**O(log n)、O(n log n)：**对数阶时间复杂度，比较常见也比较难分析。

```
i=1;
while (i <= n)  {
	i = i * 2;
} 
```

这里 i 是从 1 开始取，大于 n 时停止，实际上是一个等比数列，把它一个个列出来：

2^1^  2^2^···2^k^···2^x^ = n，其中的 x 就是这段代码的执行次数，x = log~2~n，**这里注意了！**这就是上面提到加粗 f(n) 的原因，这里 x = f(n)，是乘方的次数，刚好也是乘方这个行为执行的次数，所以时间复杂度就是 O(log~2~n)。

当循环体变成 

> i = i * 3

x = log~3~n，时间复杂度就是 O(log~3~n)。

实际上，对数阶的时间复杂度都可以记为 O(log n) （以 2 为底的对数可以不写 2 嘛），对数有换底公式和对数乘法，所以像 log~2~n、log~3~n 之类的都可以换成一个常数 C * log n 的形式，而常数，前面讲了一堆，是不影响时间复杂度的。

而 O(n log n) 就是时间复杂度为 O(log n) 的代码执行了 n 次的结果。归并排序和快速排序的时间复杂度就是 O(n log n)，当然这两种排序后面会详细讲到，现在记得这俩排序很快就行了。(看完 CS50 发现大部分情况是快排比较快的，这种排序速度的比较最好是看可视化图形，这样比较直观容易理解，youtube上有很多视频，多搜搜看看，https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html 这个网站可以参考一下) 

**O(m+n)、O(m*n)：**这样的表示说明时间复杂度是由两种代码数据规模决定的。

```
int cal(int m, int n) {
	int sum_1 = 0;
	int i = 1;
	for (; i < m; ++i) {
		sum_1 = sum_1 + i;
	}

	int sum_2 = 0;
	int j = 1;

	for (; j < n; ++j) {
		sum_2 = sum_2 + j;
	}

	return sum_1 + sum_2;
}
```

m 和 n 是函数的参数，无法事先知道量级大小，所以在表示复杂度的时候不能简单实用加法原则省略其中一个，但此时乘法原则依然适用。

-----------

除了以上列举的 常量阶 O(1)、对数阶 O(log n)、线性阶 O( n)、线性对数阶 O(n log n)、平方阶 O(n^2^) 外，还有 立方阶 O(n^3^)、k 次方阶 O(n^k^)、指数阶 O(2^n^)、阶乘阶 O(n!)。

可以粗略的分为**多项式量级**和**非多项式量级**，非多项式量级只有两个：O(2^n^) 和 O(n!)。

时间复杂度为非多项式量级的算法问题叫作 **NP** 问题。当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，执行时间会无限增长，属于非常低效的算法。

## 空间复杂度

全称叫**渐进空间复杂度**，表示算法的存储空间与数据规模增长之间的关系。

```
void print(int n) {
	int i = 0;
	int[] a = new int[n];
	for (i; i <n; ++i) {
		a[i] = i * i;
	}  

​	for (i = n-1; i >= 0; --i) {
​    	print out a[i]
​    }
}
```

变量 i ，是常量阶的，跟时间复杂度一样，跟数据规模没有关系，忽略。后面申请了一个大小为 n 的数组，剩下的代码没有占用更多空间，所以空间复杂度就是 O(n)。常见的空间复杂度是 O(1)、O(n)、O(n^2^)，对数阶的一般用不到，暂时掌握到这里。

## 时间复杂度分析进阶

四个复杂度分析方面的知识点：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。

**最好情况时间复杂度：**就是在理想情况下，比如在数组中查找元素，元素在第一个位置，此时时间复杂度是 O(1) ，剩下的 n-1 个元素就不用查找了。

**最坏情况时间复杂度：**就是在最坏的情况下，比如在数组中查找元素，而元素根本不在数组中，这样得把数组元素全都查找一遍。

以上两种情况都属于极端情况，一般不会出现。所以引入了平均情况的概念。

**平均情况时间复杂度：**还是刚才的例子，大小为 n 的数组，查找元素 x ，x 在数组中的位置共有 n+1 种，即 0～n-1 和 不在数组中。把每种情况下，查找需要遍历的元素个数累加起来，然后除以位置种数，就得到了遍历元素个数的平均值，就得到了 O(n)：

<img src="https://static001.geekbang.org/resource/image/d8/2f/d889a358b8eccc5bbb90fc16e327a22f.jpg" alt="img" style="zoom:50%;" />

但是这里的概率分析有不严谨的地方，比如，在数组中和不在数组中这两种情况的概率分别是  $1\over2$ ，如果在数组中，那么出现在 n 个位置的概率 $1\over n$ 是一样的，根据概率乘法法则，数据在 0~n-1 任意位置的概率为 $1 \over 2n$ 。如果把每种情况发生的概率考虑进去，那么平均情况时间复杂度的计算就变成了下图，结果也为 O(n)：

<img src="https://static001.geekbang.org/resource/image/36/7f/36c0aabdac69032f8a43368f5e90c67f.jpg" alt="img" style="zoom:50%;" />

这个叫加权平均值，也就是期望。因此平均情况时间复杂度又叫**加权平均时间复杂度**或者**期望时间复杂度**。

**引出这三种复杂度实际上不是需要都分析，而是因为相同的算法在不同情况下的时间复杂度量级有差距，才会用三种复杂度来区分。**

**均摊时间复杂度：**使用的分析方法叫作摊还分析，也叫平摊分析。这种复杂度的应用场景比平均时间复杂度的更加特殊、有限。

    // array表示一个长度为n的数组
    // 代码中的array.length就等于n
     int[] array = new int[n];
     int count = 0;
    
    void insert(int val) {
        if (count == array.length) {
           int sum = 0;
           for (int i = 0; i < array.length; ++i) {
              sum = sum + array[i];
           }
           array[0] = sum;
           count = 1;
        }
        
    	array[count] = val;
    	++count;
    }

这段代码功能是往数组中插入数据。对已满的数组逐项求和，并将和放到数组第一个位置，然后把新元素依次放到后面，覆盖原来数组中的内容，如果一开始数组就有空位，直接放入新元素。

这段代码，在理想情况下即数组有空位，那么最好情况时间复杂度就是 O(1)，在最坏情况下即数组已满，那么最坏情况时间复杂度及时 O(n)，因为要遍历一次数组作求和。**而此时均摊时间复杂度为 O(1)。**

如何求得？

数组长度是 n ，总共有 n 种插入位置，即 n 种情况，这 n 种情况因为数组没满不用求和所以时间复杂度都为 O(1)；还有一种额外情况，即数组满了，需要遍历求和了，此时的时间复杂度为 O(n)。这 n+1 中情况发生的概率实际上是一样的，都为 $1 \over n+1$，根据加权平均的计算方法，求得的平均时间复杂度是：

<img src="https://static001.geekbang.org/resource/image/6d/ed/6df62366a60336d9de3bc34f488d8bed.jpg" alt="img" style="zoom:50%;" />

**重头分析来了：**通过上面的分析和式子可以发现，这个算法的时间复杂度有规律，就是大部分都是 O(1) 的时间复杂度，然后出现一个 O(n) 的时间复杂度，而且，它们的出现有相对先后的时序关系，即一个求和 O(n) 后出现 n-1 个插入 O(1) ，然后循环。

对于这种特殊场景的时间复杂度分析，不需要像平均复杂度一样把所有情况的概率都求得，而是找出一般规律即可，这种分析方法就叫做**摊还分析**，得出的就叫做**均摊时间复杂度**。

每一次的求和 O(n) 都会跟着 n-1 个插入  O(1) ，所以把求和 O(n) 这次耗费最长时间的操作用时，均摊到其余的每个 O(1) 上，就相当于 n-1 个 O(1) 均摊了一个  O(n)，这样一组连续的操作就都是  O(1) 了，这就是均摊的思路。

对一个数据结构的一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况时间复杂度高，而且这些操作有相对先后的时序关系，就可以用均摊思想来分析，看看能否将时间复杂度高的操作用时均摊到其他时间复杂度低的操作上。**而且，在能够用均摊分析时间复杂度的场合，一般得出的均摊时间复杂度和最好情况时间复杂度是相同的。**

--------

之所以有这么多时间复杂度，就是因为同一个算法，在各种不同的情况下，时间复杂度的量级可能是不一样的，所以多维的分析可以更立体的展示算法的执行效率，对算法有个更好的判断。